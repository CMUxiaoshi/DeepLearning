{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBITN0M_LKds"
      },
      "source": [
        "# HW2P2: Face Classification and Verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-Mcx4Dsovv"
      },
      "source": [
        "This is the second homework in 11785: Introduction to Deep Learning consisting of two parts:\n",
        "*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7001 identities.\n",
        "*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities.\n",
        "\n",
        "For this HW, you only have to write code to implement your model architecture. Everything else has been provided for you, on the pretext that most of your time will be used up in developing the suitable model architecture for achieving satisfactory performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veIqzuIGsov2"
      },
      "source": [
        "Common errors which you may face in this homeworks (because of the size of the model)\n",
        "\n",
        "\n",
        "*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0cMmKuTswm6"
      },
      "source": [
        "# TODOs\n",
        "As you go, please read the code and keep an eye out for TODOs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI9o9SGuxXIY"
      },
      "source": [
        "**Structure of this notebook**:\n",
        "\n",
        "1. **Libraries** - just run it\n",
        "2. **Kaggle** - copy the code from HW1P2 and add a correct API key\n",
        "3. **Configs** - specify your configs\n",
        "4. **Dataset**\n",
        "  - **Datasets and Dataloaders** - it's already implemented, just specify your preferred augmentations/transformations\n",
        "  - **EDA and Viz** - just run it to check if your augmentations/transformations worked\n",
        "5. **Model Architecture** - implement and define your preferred model architecture\n",
        "6. **Loss, Optimizer, and Scheduler** - define your loss, optimizer, and a scheduler\n",
        "7. **Training and Validation Functions** - includes functions for both classification and verification tasks\n",
        "  - **Classification Task** - just run it\n",
        "  - **Verification Task** - just run it\n",
        "8. **Wandb** - add a correct API key\n",
        "9. **Experiments** - make changes if needed; it will show you train/val/test accuracy for both classification and verification. **Important**: make sure you save your best model checkpoints because models in this HW take a long time to get trained\n",
        "10. **Testing and Kaggle Submission** - just run it\n",
        "11. **Model Finetuning** - this part might help you get a higher score on the verification part\n",
        "  - **Model Implementations (CenterLoss, ArcFace, or SphereFace)** - please refer to the additional notebook and just copy the model from there\n",
        "  - **Model Definition (finetuning)** - you may reuse the code from the main part\n",
        "  - **Loss, Optimizer, and Scheduler Definition (finetuning)** - you may reuse the code from the main part\n",
        "  - **Training Function (finetuning)** - you may reuse the code from the main part\n",
        "  - **Wandb (finetuning)** - you may reuse the code from the main part\n",
        "  - **Experiments (finetuning)** - you may reuse the code from the main part\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdoDIKWOMF59"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jza7lwiScUhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4db8d5-68c6-4a00-c315-98384c25fe8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 21 22:36:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla P4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8               7W /  75W |      2MiB /  7680MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # Run this to see what GPU you have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gt7xVl4-UM_"
      },
      "source": [
        "#NOTE: RESTART THE RUN TIME AFTER RUNNING THE THE CELL BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxfd_nqFnL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec28b3d9-26e1-4b04-f523-4a6f4433ee6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
            "Requirement already satisfied: torchvision==0.13.1 in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1) (4.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary packages. Very important: Please restart your session in Colab/Local Machine\n",
        "# You can restart your session in colab by going to Runtime and then clicking restart session\n",
        "!pip install wandb --quiet\n",
        "!pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLEd0gdPbSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5586bbb-d562-4b19-9b1a-227e6d54a0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "# Import the important packages for this Homework. Feel free to add anything here you need.\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision # This library is used for image-based operations (Augmentations)\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRz9et3SZnbO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive # Link to your drive if you are not using Colab with GCP\n",
        "drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BksgPdkQwwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f924edbb-959e-4070-8284-94bda7360df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# TODO: Use the same Kaggle code from HW1P2\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"xiaocmumsppm\", \"key\":\"ff2230c7aaef22ec98252b82537c0f73\"}')\n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9BG5Gr3YIVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd06a20-dbb1-41ae-eda0-e976d8871185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/data’: File exists\n",
            "Downloading 11785-hw2p2-face-recognition.zip to /content\n",
            " 99% 1.70G/1.71G [00:09<00:00, 202MB/s]\n",
            "100% 1.71G/1.71G [00:09<00:00, 189MB/s]\n",
            "Downloading 11785-hw2p2-face-verification.zip to /content\n",
            " 77% 13.0M/16.8M [00:00<00:00, 136MB/s]\n",
            "100% 16.8M/16.8M [00:00<00:00, 153MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Reminder: Make sure you have connected your kaggle API before running this block\n",
        "!mkdir '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11785-hw2p2-face-recognition\n",
        "!unzip -qo '11785-hw2p2-face-recognition.zip' -d '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11785-hw2p2-face-verification\n",
        "!unzip -qo '11785-hw2p2-face-verification.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68hT27SXClj"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7qpMxG0XCJz"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 256, # Increase this if your GPU can handle it\n",
        "    'lr': 0.01,\n",
        "    'epochs': 60, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
        "    # Include other parameters as needed.\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSeiKHYrM-6b"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oivdYZNztqBU"
      },
      "source": [
        "## Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmRX5omaNDEZ"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Data paths\n",
        "\n",
        "DATA_DIR    = '/content/data/11-785-s24-hw2p2-classification' # TODO: Path where you have downloaded the classificaation data\n",
        "TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n",
        "VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n",
        "TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Transformations/augmentations of the Train dataset\n",
        "\n",
        "\"\"\"\n",
        "Refer https://pytorch.org/vision/stable/transforms.html\n",
        "Implementing the right train transforms/augmentation methods is key to improving performance.\n",
        "Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
        "But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
        "#Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
        "\"\"\"\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomResizedCrop(size=(224, 224), scale=(1.15, 2.15)),\n",
        "    torchvision.transforms.RandomCrop(size=(224, 224)),\n",
        "    torchvision.transforms.RandomRotation(degrees=45),\n",
        "    torchvision.transforms.ColorJitter(brightness=0.5),\n",
        "    torchvision.transforms.ColorJitter(contrast=0.5),\n",
        "    torchvision.transforms.ColorJitter(saturation=0.5),\n",
        "    torchvision.transforms.ColorJitter(hue=0.2),\n",
        "\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    ]) # TODO: Specify transformations/augmentations performed on the train dataset\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Transformations/augmentations of the Val dataset\n",
        "\n",
        "\"\"\"\n",
        "You should NOT have data augmentation on the validation set. Why?\n",
        "\"\"\"\n",
        "\n",
        "valid_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "    ]) # TODO: Specify transformations performed on the val dataset\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initializing the train and val datasets\n",
        "\n",
        "train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n",
        "valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform = valid_transforms)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initializing the train and val dataloaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset       = train_dataset,\n",
        "                                           batch_size    = config['batch_size'],\n",
        "                                           shuffle        = True,\n",
        "                                           # num_workers = 4, # Uncomment this line if you want to increase your num workers\n",
        "                                           pin_memory    = True)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset       = valid_dataset,\n",
        "                                           batch_size    = config['batch_size'],\n",
        "                                           shuffle        = False,\n",
        "                                           # num_workers = 2 # Uncomment this line if you want to increase your num workers\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqSR063BGE2e"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Test dataset class\n",
        "\n",
        "\"\"\"\n",
        "You can do this with ImageFolder as well, but it requires some tweaking\n",
        "\"\"\"\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir   = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
        "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initializing the test dataset\n",
        "\n",
        "\"\"\"\n",
        "Why are we using val_transforms for Test Data?\n",
        "\"\"\"\n",
        "\n",
        "test_dataset = TestDataset(TEST_DIR, transforms = valid_transforms)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initializing the test dataloader\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset    = test_dataset,\n",
        "                                          batch_size = config['batch_size'],\n",
        "                                          shuffle     = False,\n",
        "                                          drop_last  = False,\n",
        "                                          # num_workers = 2 # Uncomment this line if you want to increase your num workers\n",
        "                                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs2Xw_tl0IQ8"
      },
      "source": [
        "## EDA and Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4t8eU9gY0Jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4fd104-250d-4028-aab6-ace40f7c358b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes    :  7001\n",
            "No. of train images  :  140020\n",
            "Shape of image       :  torch.Size([3, 224, 224])\n",
            "Batch size           :  256\n",
            "Train batches        :  547\n",
            "Val batches          :  137\n"
          ]
        }
      ],
      "source": [
        "# Double-check your dataset/dataloaders work as expected\n",
        "\n",
        "print(\"Number of classes    : \", len(train_dataset.classes))\n",
        "print(\"No. of train images  : \", train_dataset.__len__())\n",
        "print(\"Shape of image       : \", train_dataset[0][0].shape)\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader.__len__())\n",
        "print(\"Val batches          : \", valid_loader.__len__())\n",
        "\n",
        "# Feel free to print more things if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIoRUzCbz85y"
      },
      "outputs": [],
      "source": [
        "# Visualize a few images in the dataset\n",
        "\n",
        "\"\"\"\n",
        "You can write your own code, and you don't need to understand the code\n",
        "It is highly recommended that you visualize your data augmentation as sanity check\n",
        "\"\"\"\n",
        "\n",
        "r, c    = [5, 5]\n",
        "fig, ax = plt.subplots(r, c, figsize= (15, 15))\n",
        "\n",
        "k       = 0\n",
        "dtl     = torch.utils.data.DataLoader(\n",
        "    dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = True)\n",
        "\n",
        "for data in dtl:\n",
        "    x, y = data\n",
        "\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            img = x[k].numpy().transpose(1, 2, 0)\n",
        "            ax[i, j].imshow(img)\n",
        "            ax[i, j].axis('off')\n",
        "            k+=1\n",
        "    break\n",
        "\n",
        "del dtl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyA1rPcdgvBr"
      },
      "source": [
        "FAQ:\n",
        "\n",
        "**What's a very low early deadline architecture (mandatory early submission)**?\n",
        "\n",
        "- The very low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit is 18M.\n",
        "- The first convolutional layer has 64 channels, kernel size 7, and stride 4. The next four have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2. Documentation to make convolutional layers: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "- Think about strided convolutions from the lecture, as convolutions with stride = 1 and downsampling. For strided convolution, what padding do you need for preserving the spatial resolution? (Hint => padding = kernel_size // 2) - Think why?\n",
        "- Each convolutional layer is accompanied by a Batchnorm and ReLU layer.\n",
        "- Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d. Documentation for AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n",
        "- Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "Look through https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "\n",
        "**Why does a very simple network have 4 convolutions**?\n",
        "\n",
        "Input images are 224x224. Note that each of these convolutions downsample. Downsampling 2x effectively doubles the receptive field, increasing the spatial region each pixel extracts features from. Downsampling 32x is standard for most image models.\n",
        "\n",
        "**Why does a very simple network have high channel sizes**?\n",
        "\n",
        "Every time you downsample 2x, you do 4x less computation (at same channel size). To maintain the same level of computation, you 2x increase # of channels, which increases computation by 4x. So, balances out to same computation. Another intuition is - as you downsample, you lose spatial information. We want to preserve some of it in the channel dimension.\n",
        "\n",
        "**What is return_feats?**\n",
        "\n",
        "It essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. You might also find that the classification outputs are sometimes better for verification too - try both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722d15d7-7e5a-4537-d256-0cea9b507b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "    ResidualBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "    ResidualBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "    ResidualBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "    ResidualBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "    ResidualBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "    ResidualBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "    ResidualBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "    ResidualBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "          Flatten-68                  [-1, 512]               0\n",
            "           Linear-69                 [-1, 7001]       3,591,513\n",
            "================================================================\n",
            "Total params: 14,768,089\n",
            "Trainable params: 14,768,089\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.84\n",
            "Params size (MB): 56.34\n",
            "Estimated Total Size (MB): 119.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, inchannels, out_channels, stride = 1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(inchannels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.relu = torch.nn.ReLU(inplace = True)\n",
        "\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = torch.nn.Sequential()\n",
        "\n",
        "        if stride != 1 or inchannels != out_channels:\n",
        "            self.shortcut = torch.nn.Sequential(torch.nn.Conv2d(inchannels, out_channels, stride = stride, kernel_size = 1, bias = False),\n",
        "                                                torch.nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# TODO: Fill out the model definition below\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=7001):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(64)\n",
        "        self.relu = torch.nn.ReLU(inplace = True)\n",
        "        self.maxpool = torch.nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2, 1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, 2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, 2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, 2)\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.cls_layer = torch.nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride = 1):\n",
        "        layer = []\n",
        "        layer.append(ResidualBlock(in_channels,out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layer.append(ResidualBlock(out_channels, out_channels))\n",
        "        return torch.nn.Sequential(*layer)\n",
        "\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        out = self.flatten(out)\n",
        "        out = self.cls_layer(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Initialize your model\n",
        "model = Network().to(DEVICE)\n",
        "summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Loss, Optimizer, and Scheduler Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss() # TODO: What loss do you need for a multi class classification problem and would label smoothing be beneficial here?\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                            lr=config['lr']) # TODO: Feel free to pick a different optimizer\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSh1CZMSi-Wy"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDucyAFwDXE3"
      },
      "source": [
        "## Classification Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSw6iJJavBZ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n",
        "            outputs = model(images)\n",
        "            loss    = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss      += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        # TODO? Depending on your choice of scheduler,\n",
        "        # You may want to call some schdulers inside the train function. What are these?\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5V2UdnpdEoK"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Move images to device\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmotca6pcLLY"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGZDXLAaDZ32"
      },
      "source": [
        "## Verification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "The verification task consists of the following generalized scenario:\n",
        "- You are given X unknown identitites\n",
        "- You are given Y known identitites\n",
        "- Your goal is to match X unknown identities to Y known identities.\n",
        "\n",
        "We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.\n",
        "\n",
        "Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.\n",
        "\n",
        "This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9aY5o-suWdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba0a4b4-3a41-48c0-a99e-85e6cbe201a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 360/360 [00:00<00:00, 8528.67it/s]\n",
            "100%|██████████| 720/720 [00:00<00:00, 11411.65it/s]\n",
            "100%|██████████| 960/960 [00:00<00:00, 12977.26it/s]\n"
          ]
        }
      ],
      "source": [
        "# This obtains the list of known identities from the known folder\n",
        "known_regex = \"/content/data/11-785-s24-hw2p2-verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n",
        "\n",
        "# Obtain a list of images from unknown folders\n",
        "unknown_dev_regex = \"/content/data/11-785-s24-hw2p2-verification/unknown_dev/*\"\n",
        "unknown_test_regex = \"/content/data/11-785-s24-hw2p2-verification/unknown_test/*\"\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n",
        "unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "# Why do you need only ToTensor() here?\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n",
        "unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "#Print your shapes here to understand what we have done\n",
        "\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk1LS0BRxFHM"
      },
      "outputs": [],
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):\n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "\n",
        "        with torch.no_grad():\n",
        "            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model\n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "\n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n",
        "\n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "    # Print the inner list comprehension in a separate cell - what is really happening?\n",
        "\n",
        "    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n",
        "    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n",
        "\n",
        "\n",
        "    # Note that in unknown identities, there are identities without correspondence in known identities.\n",
        "    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain\n",
        "    # threshold compared with those identities with correspondence.\n",
        "\n",
        "    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n",
        "    pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n",
        "\n",
        "    # After early submission, remove the previous line and uncomment the following code\n",
        "\n",
        "    # threshold = # Choose a proper threshold\n",
        "    # NO_CORRESPONDENCE_LABEL = 'n000000'\n",
        "    # pred_id_strings = []\n",
        "    # for idx, prediction in enumerate(predictions):\n",
        "    #     if max_similarity_values[idx] < threshold: # why < ? Thank about what is your similarity metric\n",
        "    #         pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n",
        "    #     else:\n",
        "    #         pred_id_strings.append(known_paths[prediction])\n",
        "\n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/content/data/11-785-s24-hw2p2-verification/verification_dev.csv')['label'].tolist()\n",
        "      accuracy = 100 * accuracy_score(pred_id_strings, true_ids)\n",
        "      #print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "      return accuracy, pred_id_strings\n",
        "\n",
        "    elif mode == 'test':\n",
        "      return pred_id_strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sobd9DrAcDSf"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBgKGkXLrdJ"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix62_BkaLr_D"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=\"replace with your API key here\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG0vmsmbRYEi"
      },
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkRw1FvLqYe"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZvbH_P1dIHu"
      },
      "outputs": [],
      "source": [
        "best_class_acc      = 0.0\n",
        "best_ver_acc        = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    # print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = NotImplemented # TODO: What function would you want implement here\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc (Classification) {:.04f}%\\t Train Loss (Classification) {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1, config['epochs'], train_acc, train_loss, curr_lr))\n",
        "\n",
        "    val_acc, val_loss = NotImplemented # TODO: What function would you want implement here\n",
        "    print(\"Val Acc (Classification) {:.04f}%\\t Val Loss (Classification) {:.04f}\".format(val_acc, val_loss))\n",
        "    scheduler.step(val_loss)\n",
        "    ver_acc, pred_id_strings = eval_verification(unknown_dev_images, known_images,\n",
        "                                                 model, similarity_metric, config['batch_size'], mode='val')\n",
        "\n",
        "    print(\"Val Acc (Verification) {:.04f}%\\t \".format(ver_acc))\n",
        "\n",
        "    wandb.log({\"train_classification_acc\": train_acc,\n",
        "               \"train_classification_loss\":train_loss,\n",
        "               \"val_classification_acc\": val_acc,\n",
        "               \"val_classification_loss\": val_loss,\n",
        "               \"val_verification_acc\": ver_acc,\n",
        "               \"learning_rate\": curr_lr})\n",
        "\n",
        "    # If you are using a scheduler in your train function within your iteration loop,\n",
        "    # How will you step your scheduler ?\n",
        "\n",
        "    if val_acc >= best_class_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save({'model_state_dict':model.state_dict(),\n",
        "                    'optimizer_state_dict':optimizer.state_dict(),\n",
        "                    'scheduler_state_dict':scheduler.state_dict(),\n",
        "                    'val_acc': val_acc,\n",
        "                    'epoch': epoch}, './checkpoint_classification.pth')\n",
        "        wandb.save('checkpoint_verification.pth')\n",
        "        print(\"Saved best classification model\")\n",
        "\n",
        "    if ver_acc >= best_ver_acc:\n",
        "      best_ver_acc = ver_acc\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': ver_acc,\n",
        "                  'epoch': epoch}, './checkpoint_verification.pth')\n",
        "      wandb.save('checkpoint_verification.pth')\n",
        "      print(\"Saved verification model\")\n",
        "\n",
        "### Finish your wandb run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Testing and Kaggle Submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"content\\ResNet18.pth\"\n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "EH2Kp9Zx4JO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/ResNet18.pth\"\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDrR6pRj4UXT",
        "outputId": "d633f2d1-8d66-48d1-c38c-85b7a2feec18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (cls_layer): Linear(in_features=512, out_features=7001, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo"
      },
      "outputs": [],
      "source": [
        "def test(model,dataloader): # TODO: Run to finish predicting on the test set.\n",
        "\n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "  test_results = []\n",
        "\n",
        "  for i, (images) in enumerate(dataloader):\n",
        "\n",
        "      images = images.to(DEVICE)\n",
        "\n",
        "      with torch.inference_mode():\n",
        "        outputs = model(images)\n",
        "\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
        "      test_results.extend(outputs)\n",
        "\n",
        "      batch_bar.update()\n",
        "\n",
        "  batch_bar.close()\n",
        "  return test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7R1lcCAzULc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247921df-cc9c-4397-ee11-1f9e60601e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "test_results = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmejXfRh5YRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f091551-fa5e-4916-bd75-6881a9d20dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "pred_id_strings = eval_verification(unknown_test_images, known_images,\n",
        "                                                 model, similarity_metric, config['batch_size'], mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "# TODO: Run to write the results in the files and submit to Kaggle\n",
        "# You only have 10 Kaggle submissions per day\n",
        "\n",
        "with open(\"classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))\n",
        "\n",
        "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(pred_id_strings)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aXd-fzh2h3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b76481-7677-464f-a8e5-be1dfa669c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.6 / client 1.5.8)\n",
            "100% 541k/541k [00:00<00:00, 625kB/s]\n",
            "Successfully submitted to 11785 HW2P2 - Face Recognition Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.6 / client 1.5.8)\n",
            "100% 8.34k/8.34k [00:00<00:00, 8.75kB/s]\n",
            "Successfully submitted to 11785 HW2P2 - Face Verification"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11785-hw2p2-face-recognition -f classification_early_submission.csv -m \"Early-Submission\"\n",
        "!kaggle competitions submit -c 11785-hw2p2-face-verification -f verification_early_submission.csv -m \"Early-Submission\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9VKGDekKUi"
      },
      "source": [
        "# Finetune your Model\n",
        "\n",
        "You can choose any model for finetuning. It is a good practice to try to sweep through all the models to find what is the best finetuning model for you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UofR6hvFHiCx"
      },
      "source": [
        "## Model Definition (finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDvBhRMtEFCz"
      },
      "outputs": [],
      "source": [
        "# add your finetune/retrain code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRGArZF4Hu5o"
      },
      "source": [
        "## Loss, Optimizer, and Scheduler Definition (finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB9LyMz_EHCO"
      },
      "outputs": [],
      "source": [
        "# add your finetune/retrain code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLyEgTOYItfC"
      },
      "source": [
        "## Training Function (finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWA_qrPVEHnT"
      },
      "outputs": [],
      "source": [
        "# add your finetune/retrain code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHYLxuPIen7"
      },
      "source": [
        "## Wandb (finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HirsRgMgEICs"
      },
      "outputs": [],
      "source": [
        "# add your finetune/retrain code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRTN2GVxIiHP"
      },
      "source": [
        "## Experiments (finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AgltCfukKUl"
      },
      "outputs": [],
      "source": [
        "# add your finetune/retrain code here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lDucyAFwDXE3",
        "fGZDXLAaDZ32",
        "2mBgKGkXLrdJ",
        "df9VKGDekKUi"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}