{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e25d072-2967-405e-826f-e141fbf430a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb torchsummary pandas scikit-learn --quiet\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision # This library is used for image-based operations (Augmentations)\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import glob\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2174d37-a4aa-4bc3-b96d-49b556b0236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the same Kaggle code from HW1P2\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"xiaocmumsppm\", \"key\":\"6a25da7cd718a7252a65bb3a7658930a\"}')\n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5916245-014e-4d73-b957-7947d84ca4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir '/root/autodl-tmp/data'\n",
    "\n",
    "# !kaggle competitions download -c 11785-hw2p2-face-recognition\n",
    "!unzip -qo '/root/autodl-tmp/11785-hw2p2-face-recognition.zip' -d '/root/autodl-tmp'\n",
    "\n",
    "# !kaggle competitions download -c 11785-hw2p2-face-verification\n",
    "!unzip -qo '/root/autodl-tmp/11785-hw2p2-face-verification.zip' -d '/root/autodl-tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ec7757-b28c-4bbd-aabe-279266bb9499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 13 07:55:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:16:00.0 Off |                  Off |\n",
      "| 30%   27C    P8              15W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:27:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              19W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        On  | 00000000:38:00.0 Off |                  Off |\n",
      "| 30%   40C    P8              14W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        On  | 00000000:5A:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              21W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        On  | 00000000:98:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              20W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        On  | 00000000:A8:00.0 Off |                  Off |\n",
      "| 30%   30C    P8              14W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 4090        On  | 00000000:B8:00.0 Off |                  Off |\n",
      "| 30%   27C    P8              12W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce RTX 4090        On  | 00000000:D8:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              20W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # Run this to see what GPU you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0249dc4-2106-4e64-a5b6-bd03565cdc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 256, # Increase this if your GPU can handle it\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 150, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
    "    # Include other parameters as needed.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bc42b7-188c-41f4-a22c-09b17f9de1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------- #\n",
    "\n",
    "# Data paths\n",
    "\n",
    "DATA_DIR    = \"/root/autodl-tmp/11-785-s24-hw2p2-classification\" # TODO: Path where you have downloaded the classificaation data\n",
    "TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n",
    "TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Transformations/augmentations of the Train dataset\n",
    "\n",
    "\"\"\"\n",
    "Refer https://pytorch.org/vision/stable/transforms.html\n",
    "Implementing the right train transforms/augmentation methods is key to improving performance.\n",
    "Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
    "But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
    "#Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
    "\"\"\"\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), \n",
    "    torchvision.transforms.RandomPerspective(distortion_scale=0.2, p=0.5, interpolation=3),\n",
    "    torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1)), \n",
    "\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5103, 0.4014, 0.3508], std=[0.3077, 0.2701, 0.2591])\n",
    "    ]) # TODO: Specify transformations/augmentations performed on the train dataset\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Transformations/augmentations of the Val dataset\n",
    "\n",
    "\"\"\"\n",
    "You should NOT have data augmentation on the validation set. Why?\n",
    "\"\"\"\n",
    "\n",
    "valid_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5103, 0.4014, 0.3508], std=[0.3077, 0.2701, 0.2591])\n",
    "    ]) # TODO: Specify transformations performed on the val dataset\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Initializing the train and val datasets\n",
    "\n",
    "train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n",
    "valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform = valid_transforms)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Initializing the train and val dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset       = train_dataset,\n",
    "                                           batch_size    = config['batch_size'],\n",
    "                                           shuffle        = True,\n",
    "                                           num_workers = 8, # Uncomment this line if you want to increase your num workers\n",
    "                                           pin_memory = True\n",
    "                                           )\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset       = valid_dataset,\n",
    "                                           batch_size    = 1024,\n",
    "                                           shuffle        = False,\n",
    "                                           num_workers = 128, # Uncomment this line if you want to increase your num workers\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a41bdd-ac22-4784-9b15-e7ba5913d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------- #\n",
    "\n",
    "# Test dataset class\n",
    "\n",
    "\"\"\"\n",
    "You can do this with ImageFolder as well, but it requires some tweaking\n",
    "\"\"\"\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, transforms):\n",
    "        self.data_dir   = data_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
    "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transforms(Image.open(self.img_paths[idx]))\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Initializing the test dataset\n",
    "\n",
    "\"\"\"\n",
    "Why are we using val_transforms for Test Data?\n",
    "\"\"\"\n",
    "\n",
    "test_dataset = TestDataset(TEST_DIR, transforms = valid_transforms)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Initializing the test dataloader\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset    = test_dataset,\n",
    "                                          batch_size = 1024,\n",
    "                                          shuffle     = False,\n",
    "                                          drop_last  = False,\n",
    "                                          num_workers = 128,\n",
    "                                          # Uncomment this line if you want to increase your num workers\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490d3ee2-55ef-40af-85cb-723901200459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes    :  7001\n",
      "No. of train images  :  140020\n",
      "Shape of image       :  torch.Size([3, 224, 224])\n",
      "Batch size           :  256\n",
      "Train batches        :  547\n",
      "Val batches          :  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes    : \", len(train_dataset.classes))\n",
    "print(\"No. of train images  : \", train_dataset.__len__())\n",
    "print(\"Shape of image       : \", train_dataset[0][0].shape)\n",
    "print(\"Batch size           : \", config['batch_size'])\n",
    "print(\"Train batches        : \", train_loader.__len__())\n",
    "print(\"Val batches          : \", valid_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61d8b60-cc68-4f74-adab-c7020ed3e02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 188, 56, 56]           9,212\n",
      "            Conv2d-2          [-1, 188, 56, 56]           9,212\n",
      "         LayerNorm-3          [-1, 188, 56, 56]             376\n",
      "         LayerNorm-4          [-1, 188, 56, 56]             376\n",
      "            Conv2d-5          [-1, 188, 56, 56]           9,400\n",
      "            Conv2d-6          [-1, 188, 56, 56]           9,400\n",
      "         LayerNorm-7          [-1, 56, 56, 188]             376\n",
      "         LayerNorm-8          [-1, 56, 56, 188]             376\n",
      "            Linear-9          [-1, 56, 56, 752]         142,128\n",
      "           Linear-10          [-1, 56, 56, 752]         142,128\n",
      "             GELU-11          [-1, 56, 56, 752]               0\n",
      "             GELU-12          [-1, 56, 56, 752]               0\n",
      "           Linear-13          [-1, 56, 56, 188]         141,564\n",
      "           Linear-14          [-1, 56, 56, 188]         141,564\n",
      "         DropPath-15          [-1, 188, 56, 56]               0\n",
      "         DropPath-16          [-1, 188, 56, 56]               0\n",
      "            Block-17          [-1, 188, 56, 56]               0\n",
      "            Block-18          [-1, 188, 56, 56]               0\n",
      "           Conv2d-19          [-1, 188, 56, 56]           9,400\n",
      "           Conv2d-20          [-1, 188, 56, 56]           9,400\n",
      "        LayerNorm-21          [-1, 56, 56, 188]             376\n",
      "        LayerNorm-22          [-1, 56, 56, 188]             376\n",
      "           Linear-23          [-1, 56, 56, 752]         142,128\n",
      "           Linear-24          [-1, 56, 56, 752]         142,128\n",
      "             GELU-25          [-1, 56, 56, 752]               0\n",
      "             GELU-26          [-1, 56, 56, 752]               0\n",
      "           Linear-27          [-1, 56, 56, 188]         141,564\n",
      "           Linear-28          [-1, 56, 56, 188]         141,564\n",
      "         DropPath-29          [-1, 188, 56, 56]               0\n",
      "            Block-30          [-1, 188, 56, 56]               0\n",
      "         DropPath-31          [-1, 188, 56, 56]               0\n",
      "            Block-32          [-1, 188, 56, 56]               0\n",
      "        LayerNorm-33          [-1, 188, 56, 56]             376\n",
      "        LayerNorm-34          [-1, 188, 56, 56]             376\n",
      "           Conv2d-35          [-1, 286, 28, 28]         215,358\n",
      "           Conv2d-36          [-1, 286, 28, 28]         215,358\n",
      "           Conv2d-37          [-1, 286, 28, 28]          14,300\n",
      "           Conv2d-38          [-1, 286, 28, 28]          14,300\n",
      "        LayerNorm-39          [-1, 28, 28, 286]             572\n",
      "        LayerNorm-40          [-1, 28, 28, 286]             572\n",
      "           Linear-41         [-1, 28, 28, 1144]         328,328\n",
      "           Linear-42         [-1, 28, 28, 1144]         328,328\n",
      "             GELU-43         [-1, 28, 28, 1144]               0\n",
      "             GELU-44         [-1, 28, 28, 1144]               0\n",
      "           Linear-45          [-1, 28, 28, 286]         327,470\n",
      "           Linear-46          [-1, 28, 28, 286]         327,470\n",
      "         DropPath-47          [-1, 286, 28, 28]               0\n",
      "            Block-48          [-1, 286, 28, 28]               0\n",
      "           Conv2d-49          [-1, 286, 28, 28]          14,300\n",
      "         DropPath-50          [-1, 286, 28, 28]               0\n",
      "            Block-51          [-1, 286, 28, 28]               0\n",
      "           Conv2d-52          [-1, 286, 28, 28]          14,300\n",
      "        LayerNorm-53          [-1, 28, 28, 286]             572\n",
      "           Linear-54         [-1, 28, 28, 1144]         328,328\n",
      "             GELU-55         [-1, 28, 28, 1144]               0\n",
      "        LayerNorm-56          [-1, 28, 28, 286]             572\n",
      "           Linear-57          [-1, 28, 28, 286]         327,470\n",
      "           Linear-58         [-1, 28, 28, 1144]         328,328\n",
      "             GELU-59         [-1, 28, 28, 1144]               0\n",
      "           Linear-60          [-1, 28, 28, 286]         327,470\n",
      "         DropPath-61          [-1, 286, 28, 28]               0\n",
      "            Block-62          [-1, 286, 28, 28]               0\n",
      "         DropPath-63          [-1, 286, 28, 28]               0\n",
      "            Block-64          [-1, 286, 28, 28]               0\n",
      "        LayerNorm-65          [-1, 286, 28, 28]             572\n",
      "        LayerNorm-66          [-1, 286, 28, 28]             572\n",
      "           Conv2d-67          [-1, 386, 14, 14]         441,970\n",
      "           Conv2d-68          [-1, 386, 14, 14]          19,300\n",
      "        LayerNorm-69          [-1, 14, 14, 386]             772\n",
      "           Linear-70         [-1, 14, 14, 1544]         597,528\n",
      "             GELU-71         [-1, 14, 14, 1544]               0\n",
      "           Linear-72          [-1, 14, 14, 386]         596,370\n",
      "         DropPath-73          [-1, 386, 14, 14]               0\n",
      "            Block-74          [-1, 386, 14, 14]               0\n",
      "           Conv2d-75          [-1, 386, 14, 14]          19,300\n",
      "        LayerNorm-76          [-1, 14, 14, 386]             772\n",
      "           Linear-77         [-1, 14, 14, 1544]         597,528\n",
      "             GELU-78         [-1, 14, 14, 1544]               0\n",
      "           Linear-79          [-1, 14, 14, 386]         596,370\n",
      "         DropPath-80          [-1, 386, 14, 14]               0\n",
      "            Block-81          [-1, 386, 14, 14]               0\n",
      "           Conv2d-82          [-1, 386, 14, 14]          19,300\n",
      "        LayerNorm-83          [-1, 14, 14, 386]             772\n",
      "           Linear-84         [-1, 14, 14, 1544]         597,528\n",
      "             GELU-85         [-1, 14, 14, 1544]               0\n",
      "           Linear-86          [-1, 14, 14, 386]         596,370\n",
      "         DropPath-87          [-1, 386, 14, 14]               0\n",
      "            Block-88          [-1, 386, 14, 14]               0\n",
      "        LayerNorm-89          [-1, 386, 14, 14]             772\n",
      "           Conv2d-90            [-1, 788, 7, 7]       1,217,460\n",
      "           Conv2d-91            [-1, 788, 7, 7]          39,400\n",
      "        LayerNorm-92            [-1, 7, 7, 788]           1,576\n",
      "           Linear-93           [-1, 7, 7, 3152]       2,486,928\n",
      "             GELU-94           [-1, 7, 7, 3152]               0\n",
      "           Linear-95            [-1, 7, 7, 788]       2,484,564\n",
      "         DropPath-96            [-1, 788, 7, 7]               0\n",
      "            Block-97            [-1, 788, 7, 7]               0\n",
      "AdaptiveAvgPool2d-98            [-1, 788, 1, 1]               0\n",
      "          Flatten-99                  [-1, 788]               0\n",
      "       LayerNorm-100                  [-1, 788]           1,576\n",
      "          Linear-101                 [-1, 7001]       5,523,789\n",
      "        ConvNeXt-102                 [-1, 7001]               0\n",
      "          Conv2d-103          [-1, 386, 14, 14]         441,970\n",
      "          Conv2d-104          [-1, 386, 14, 14]          19,300\n",
      "       LayerNorm-105          [-1, 14, 14, 386]             772\n",
      "          Linear-106         [-1, 14, 14, 1544]         597,528\n",
      "            GELU-107         [-1, 14, 14, 1544]               0\n",
      "          Linear-108          [-1, 14, 14, 386]         596,370\n",
      "        DropPath-109          [-1, 386, 14, 14]               0\n",
      "           Block-110          [-1, 386, 14, 14]               0\n",
      "          Conv2d-111          [-1, 386, 14, 14]          19,300\n",
      "       LayerNorm-112          [-1, 14, 14, 386]             772\n",
      "          Linear-113         [-1, 14, 14, 1544]         597,528\n",
      "            GELU-114         [-1, 14, 14, 1544]               0\n",
      "          Linear-115          [-1, 14, 14, 386]         596,370\n",
      "        DropPath-116          [-1, 386, 14, 14]               0\n",
      "           Block-117          [-1, 386, 14, 14]               0\n",
      "          Conv2d-118          [-1, 386, 14, 14]          19,300\n",
      "       LayerNorm-119          [-1, 14, 14, 386]             772\n",
      "          Linear-120         [-1, 14, 14, 1544]         597,528\n",
      "            GELU-121         [-1, 14, 14, 1544]               0\n",
      "          Linear-122          [-1, 14, 14, 386]         596,370\n",
      "        DropPath-123          [-1, 386, 14, 14]               0\n",
      "           Block-124          [-1, 386, 14, 14]               0\n",
      "       LayerNorm-125          [-1, 386, 14, 14]             772\n",
      "          Conv2d-126            [-1, 788, 7, 7]       1,217,460\n",
      "          Conv2d-127            [-1, 788, 7, 7]          39,400\n",
      "       LayerNorm-128            [-1, 7, 7, 788]           1,576\n",
      "          Linear-129           [-1, 7, 7, 3152]       2,486,928\n",
      "            GELU-130           [-1, 7, 7, 3152]               0\n",
      "          Linear-131            [-1, 7, 7, 788]       2,484,564\n",
      "        DropPath-132            [-1, 788, 7, 7]               0\n",
      "           Block-133            [-1, 788, 7, 7]               0\n",
      "AdaptiveAvgPool2d-134            [-1, 788, 1, 1]               0\n",
      "         Flatten-135                  [-1, 788]               0\n",
      "       LayerNorm-136                  [-1, 788]           1,576\n",
      "          Linear-137                 [-1, 7001]       5,523,789\n",
      "        ConvNeXt-138                 [-1, 7001]               0\n",
      "================================================================\n",
      "Total params: 35,988,230\n",
      "Trainable params: 35,988,230\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 412.51\n",
      "Params size (MB): 137.28\n",
      "Estimated Total Size (MB): 550.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(normalized_shape))               # TODO: See if initializing weight to be 0 can improve model\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(torch.nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_prob=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = torch.nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = torch.nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.pwconv2 = torch.nn.Linear(4 * dim, dim)\n",
    "        self.gamma = torch.nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_prob) if drop_prob > 0. else torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "    \n",
    "class ConvNeXt(torch.nn.Module):    # [96, 192, 384, 768]\n",
    "    def __init__(self, num_blocks=[2, 2, 3, 1], num_channels=[188, 286, 386, 788], num_classes=7001, layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        stages = []\n",
    "        for i in range(len(num_blocks)):\n",
    "            for _ in range(num_blocks[i]):\n",
    "                stages.append([])\n",
    "                stages[i].append(Block(num_channels[i], layer_scale_init_value))\n",
    "                \n",
    "        self.stem = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, num_channels[0], kernel_size=4, stride=4), \n",
    "            LayerNorm(num_channels[0], data_format='channels_first')\n",
    "        )\n",
    "        self.stage0 = torch.nn.Sequential(*stages[0])\n",
    "        self.dimension0 = torch.nn.Sequential(\n",
    "            LayerNorm(num_channels[0], data_format='channels_first'),\n",
    "            torch.nn.Conv2d(num_channels[0], num_channels[1], kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.stage1 = torch.nn.Sequential(*stages[1])\n",
    "        self.dimension1 = torch.nn.Sequential(\n",
    "            LayerNorm(num_channels[1], data_format='channels_first'),\n",
    "            torch.nn.Conv2d(num_channels[1], num_channels[2], kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.stage2 = torch.nn.Sequential(*stages[2])\n",
    "        self.dimension2 = torch.nn.Sequential(\n",
    "            LayerNorm(num_channels[2], data_format='channels_first'),\n",
    "            torch.nn.Conv2d(num_channels[2], num_channels[3], kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.stage3 = torch.nn.Sequential(*stages[3])\n",
    "        \n",
    "        self.one_pooling = torch.nn.AdaptiveAvgPool2d((1, 1))      \n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.final_norm = torch.nn.LayerNorm(num_channels[-1], eps=1e-6)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x, return_feats=False):\n",
    "        out = self.stem(x)\n",
    "        out = self.stage0(out)\n",
    "        out = self.dimension0(out)\n",
    "        out = self.stage1(out)\n",
    "        out = self.dimension1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.dimension2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.one_pooling(out)\n",
    "        out = self.flatten(out)\n",
    "        # if return_feats:\n",
    "        #     return out\n",
    "        out = self.final_norm(out)\n",
    "        if return_feats:\n",
    "            return out                  # TODO: See if this works better\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = ConvNeXt()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to('cuda')\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdde0be2-9375-4103-9df8-6af5b8f58098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight_decay(net, l2_value, skip_list=()):\n",
    "    decay, no_decay = [], []\n",
    "    for name, param in net.named_parameters():\n",
    "        if not param.requires_grad: continue # frozen weights\t\t            \n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list: no_decay.append(param)\n",
    "        else: decay.append(param)\n",
    "    return [{'params': no_decay, 'weight_decay': 0.}, {'params': decay, 'weight_decay': l2_value}]\n",
    "\n",
    "params = add_weight_decay(model, 0.15)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "# Defining Loss function\n",
    "# cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Defining Optimizer\n",
    "# SGD optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = config['lr'], momentum = 0.9, weight_decay = 5e-4)\n",
    "# AdamW\n",
    "optimizer = torch.optim.AdamW(params, lr=config['lr'])\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Defining Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, min_lr=1e-5)# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
    "\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
    "# It is useful only in the case of compatible GPUs such as T4/V100\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c12d7a-8d7c-417e-bef5-655bd8eac605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Progress Bar\n",
    "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss  = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n",
    "            outputs = model(images)\n",
    "            loss    = criterion(outputs, labels)\n",
    "            # l2 regularization for the final layer\n",
    "            # l2_reg = torch.tensor(0.).to(DEVICE)\n",
    "            # for param in model.cls_layer.parameters():\n",
    "            #     l2_reg += torch.norm(param)\n",
    "            # loss += 0.0005 * l2_reg\n",
    "            \n",
    "        # Update no. of correct predictions & loss as we iterate\n",
    "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
    "        total_loss      += float(loss.item())\n",
    "\n",
    "        # tqdm lets you add some details so you can monitor training as you train.\n",
    "        batch_bar.set_postfix(\n",
    "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
    "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            num_correct = num_correct,\n",
    "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
    "        )\n",
    "\n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update()\n",
    "\n",
    "        # TODO? Depending on your choice of scheduler,\n",
    "        # You may want to call some schdulers inside the train function. What are these?\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "\n",
    "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
    "    total_loss  = float(total_loss / len(dataloader))\n",
    "\n",
    "    return acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c84aec1-a296-48b7-928c-b5f0f1b9aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
    "\n",
    "    num_correct = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        # Move images to device\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Get model outputs\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "\n",
    "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * num_correct / (1024 *(i + 1))),\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            num_correct=num_correct)\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "    acc = 100 * num_correct / (1024 * len(dataloader))\n",
    "    total_loss = float(total_loss / len(dataloader))\n",
    "    return acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e823dfd-cf97-405f-a612-efdcadf70355",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b1acdf-65ba-4fee-b1bb-27cc3d3def49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:00<00:00, 7877.20it/s]\n",
      "100%|██████████| 720/720 [00:00<00:00, 8074.73it/s]\n",
      "100%|██████████| 960/960 [00:00<00:00, 9242.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# This obtains the list of known identities from the known folder\n",
    "known_regex = \"autodl-tmp/11-785-s24-hw2p2-verification/known/*/*\"\n",
    "# known_regex = \"C:\\\\Users\\\\sx119\\\\Desktop\\\\11785data\\\\data\\\\11-785-s24-hw2p2-verification\\\\known\\\\*\\\\*\"\n",
    "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n",
    "# known_paths = [i.split('\\\\')[-2] for i in sorted(glob.glob(known_regex))]\n",
    "\n",
    "# Obtain a list of images from unknown folders\n",
    "unknown_dev_regex = \"autodl-tmp/11-785-s24-hw2p2-verification/unknown_dev/*\"\n",
    "# unknown_dev_regex = \"C:\\\\Users\\\\sx119\\\\Desktop\\\\11785data\\\\data\\\\11-785-s24-hw2p2-verification\\\\unknown_dev\\\\*\"\n",
    "unknown_test_regex = \"autodl-tmp/11-785-s24-hw2p2-verification/unknown_test/*\"\n",
    "# unknown_test_regex = \"C:\\\\Users\\\\sx119\\\\Desktop\\\\11785data\\\\data\\\\11-785-s24-hw2p2-verification\\\\unknown_test\\\\*\"\n",
    "\n",
    "# We load the images from known and unknown folders\n",
    "unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n",
    "unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n",
    "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
    "\n",
    "# Why do you need only ToTensor() here?\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Normalize(mean=[0.5103, 0.4014, 0.3508], std=[0.3077, 0.2701, 0.2591])])\n",
    "\n",
    "unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n",
    "unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n",
    "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
    "#Print your shapes here to understand what we have done\n",
    "\n",
    "# You can use other similarity metrics like Euclidean Distance if you wish\n",
    "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e48d6edd-7898-4326-9646-ee9c8ffc94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):\n",
    "\n",
    "    unknown_feats, known_feats = [], []\n",
    "\n",
    "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
    "    model.eval()\n",
    "\n",
    "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
    "    for i in range(0, unknown_images.shape[0], batch_size):\n",
    "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model\n",
    "        unknown_feats.append(unknown_feat)\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
    "\n",
    "    for i in range(0, known_images.shape[0], batch_size):\n",
    "        known_batch = known_images[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n",
    "\n",
    "        known_feats.append(known_feat)\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
    "    known_feats = torch.cat(known_feats, dim=0)\n",
    "\n",
    "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
    "    # Print the inner list comprehension in a separate cell - what is really happening?\n",
    "\n",
    "    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n",
    "    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n",
    "\n",
    "\n",
    "    # Note that in unknown identities, there are identities without correspondence in known identities.\n",
    "    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain\n",
    "    # threshold compared with those identities with correspondence.\n",
    "\n",
    "    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n",
    "    pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n",
    "\n",
    "    # After early submission, remove the previous line and uncomment the following code\n",
    "\n",
    "    threshold = 0.35 # You can change this threshold\n",
    "    NO_CORRESPONDENCE_LABEL = 'n000000'\n",
    "    pred_id_strings = []\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        if max_similarity_values[idx] < threshold: # why < ? Thank about what is your similarity metric\n",
    "            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n",
    "        else:\n",
    "            pred_id_strings.append(known_paths[prediction])\n",
    "\n",
    "    if mode == 'val':\n",
    "      # true_ids = pd.read_csv('C:\\\\Users\\\\sx119\\\\Desktop\\\\11785data\\\\data\\\\11-785-s24-hw2p2-verification\\\\verification_dev.csv')['label'].tolist()\n",
    "      true_ids = pd.read_csv('autodl-tmp/11-785-s24-hw2p2-verification/verification_dev.csv')['label'].tolist()\n",
    "      accuracy = 100 * accuracy_score(pred_id_strings, true_ids)\n",
    "      #print(\"Verification Accuracy = {}\".format(accuracy))\n",
    "      return accuracy, pred_id_strings\n",
    "\n",
    "    elif mode == 'test':\n",
    "      return pred_id_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e6053f-551d-4a77-adad-b231870a6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8db2ca6-8372-4641-940b-f42a194caae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcmuxiaoshi\u001b[0m (\u001b[33m11785hw2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"55110248ba20ab2e47033c657a7fccb4fae4a83d\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72468447-c4ac-4a72-ab28-fbac4587328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8685b46100a54108bf1e0d2bbd3c4f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112410321624742, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20240314_190252-nfsjwu7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/11785hw2/hw2p2-ablations/runs/nfsjwu7o' target=\"_blank\">Convnext_version12</a></strong> to <a href='https://wandb.ai/11785hw2/hw2p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/11785hw2/hw2p2-ablations' target=\"_blank\">https://wandb.ai/11785hw2/hw2p2-ablations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/11785hw2/hw2p2-ablations/runs/nfsjwu7o' target=\"_blank\">https://wandb.ai/11785hw2/hw2p2-ablations/runs/nfsjwu7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "\n",
    "run = wandb.init(\n",
    "    name = \"Convnext_version12\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d3907-79a2-4996-81ca-52ffda328a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/150: \n",
      "Train Acc (Classification) 0.0236%\t Train Loss (Classification) 8.9041\t Learning Rate 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc (Classification) 0.0167%\t Val Loss (Classification) 8.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc (Verification) 0.0000%\t \n",
      "Saved best classification model\n",
      "Saved verification model\n",
      "\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/150: \n",
      "Train Acc (Classification) 0.0457%\t Train Loss (Classification) 8.6805\t Learning Rate 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc (Classification) 0.1228%\t Val Loss (Classification) 8.5198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc (Verification) 0.0000%\t \n",
      "Saved best classification model\n",
      "Saved verification model\n",
      "\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   9%|▉         | 50/547 [00:12<02:00,  4.12it/s, acc=0.1328%, loss=8.4688, lr=0.0010, num_correct=17]"
     ]
    }
   ],
   "source": [
    "best_class_acc      = 0.0\n",
    "best_ver_acc        = 0.0\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Acc (Classification) {:.04f}%\\t Train Loss (Classification) {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        epoch + 1, config['epochs'], train_acc, train_loss, curr_lr))\n",
    "\n",
    "    val_acc, val_loss = validate(model, valid_loader, criterion)\n",
    "    print(\"Val Acc (Classification) {:.04f}%\\t Val Loss (Classification) {:.04f}\".format(val_acc, val_loss))\n",
    "    scheduler.step(val_loss)\n",
    "    ver_acc, pred_id_strings = eval_verification(unknown_dev_images, known_images,\n",
    "                                                 model, similarity_metric, config['batch_size'], mode='val')\n",
    "\n",
    "    print(\"Val Acc (Verification) {:.04f}%\\t \".format(ver_acc))\n",
    "\n",
    "    wandb.log({\"train_classification_acc\": train_acc,\n",
    "                \"train_classification_loss\":train_loss,\n",
    "                \"val_classification_acc\": val_acc,\n",
    "                \"val_classification_loss\": val_loss,\n",
    "                \"val_verification_acc\": ver_acc,\n",
    "                \"learning_rate\": curr_lr})\n",
    "\n",
    "    # If you are using a scheduler in your train function within your iteration loop,\n",
    "    # How will you step your scheduler ?\n",
    "    \n",
    "\n",
    "    if val_acc >= best_class_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        torch.save({'model_state_dict':model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict(),\n",
    "                    'scheduler_state_dict':scheduler.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'epoch': epoch}, './checkpoint_classification.pth')\n",
    "        # wandb.save('checkpoint_verification.pth')\n",
    "        print(\"Saved best classification model\")\n",
    "\n",
    "    if ver_acc >= best_ver_acc:\n",
    "        best_ver_acc = ver_acc\n",
    "        torch.save({'model_state_dict':model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict(),\n",
    "                    'scheduler_state_dict':scheduler.state_dict(),\n",
    "                    'val_acc': ver_acc,\n",
    "                    'epoch': epoch}, './checkpoint_verification.pth')\n",
    "        # wandb.save('checkpoint_verification.pth')\n",
    "        print(\"Saved verification model\")\n",
    "\n",
    "### Finish your wandb run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed77b8-27f5-47b2-88c1-46952c8c1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f635529a-4ec2-48a4-95ee-3dea883c12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataloader): # TODO: Run to finish predicting on the test set.\n",
    "\n",
    "  model.eval()\n",
    "  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
    "  test_results = []\n",
    "\n",
    "  for i, (images) in enumerate(dataloader):\n",
    "\n",
    "      images = images.to(DEVICE)\n",
    "\n",
    "      with torch.inference_mode():\n",
    "        outputs = model(images)\n",
    "\n",
    "      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
    "      test_results.extend(outputs)\n",
    "\n",
    "      batch_bar.update()\n",
    "\n",
    "  batch_bar.close()\n",
    "  return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a26fc68-77ec-4ab1-b6c4-a6eff00d8185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ConvNeXt(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(3, 188, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm()\n",
       "    )\n",
       "    (stage0): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(188, 188, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=188)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=188, out_features=752, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=752, out_features=188, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(188, 188, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=188)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=188, out_features=752, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=752, out_features=188, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "    )\n",
       "    (dimension0): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(188, 286, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(286, 286, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=286)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=286, out_features=1144, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1144, out_features=286, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(286, 286, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=286)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=286, out_features=1144, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1144, out_features=286, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "    )\n",
       "    (dimension1): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(286, 386, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(386, 386, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=386)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=386, out_features=1544, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1544, out_features=386, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(386, 386, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=386)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=386, out_features=1544, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1544, out_features=386, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(386, 386, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=386)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=386, out_features=1544, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1544, out_features=386, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "    )\n",
       "    (dimension2): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(386, 788, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(788, 788, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=788)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=788, out_features=3152, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=3152, out_features=788, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "      )\n",
       "    )\n",
       "    (one_pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (final_norm): LayerNorm((788,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=788, out_features=7001, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"checkpoint_classification.pth\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267b96e5-85a2-46f5-8515-92434473e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "test_results = test(model, test_loader)\n",
    "pred_id_strings = eval_verification(unknown_test_images, known_images,\n",
    "                                                 model, similarity_metric, config['batch_size'], mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84aa4aa2-6c4e-4cc3-960b-70579c371adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run to write the results in the files and submit to Kaggle\n",
    "# You only have 10 Kaggle submissions per day\n",
    "\n",
    "with open(\"classification_early_submission.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(test_dataset)):\n",
    "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))\n",
    "\n",
    "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(pred_id_strings)):\n",
    "        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "418d3a5d-09a7-4c4f-8d64-e2c6e8fdf14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03857908-89a9-41aa-9570-51eb5ea7d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.7 / client 1.5.8)\n",
      "  0%|                                                | 0.00/541k [00:00<?, ?B/s]"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c 11785-hw2p2-face-recognition-slack -f classification_early_submission.csv -m \"Message\"\n",
    "!kaggle competitions submit -c 11785-hw2p2-face-verification-slack -f verification_early_submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d607aea-9628-4d5e-b0ef-2ac36f5584d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
